# apps/alloy-ui/application.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: alloy-ui
  namespace: argocd
  annotations: { argocd.argoproj.io/sync-wave: "2" }
spec:
  project: default
  dependsOn:
    - monitoring                         # wait until Alloy agents exist
  destination:
    name: in-cluster
    namespace: monitoring

  # --- this is the bit that was wrong ---
  source:
    repoURL: https://grafana.github.io/helm-charts
    chart: alloy                          # <-- official chart
    targetRevision: "0.*"                 # pin a major version
    helm:
      values: |
ui:
  enabled: true
service:
  type: ClusterIP
ingress:
  enabled: true
  ingressClassName: nginx
  hosts:
    - alloy.localtest.me
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/proxy-body-size: 10m

configMap:
  create: true
  content: |-
    # Receive OTLP data (logs, metrics, traces) via HTTP and gRPC
    otelcol.receiver.otlp "default" {
      http {}
      grpc {}
      output {
        logs    = [otelcol.processor.batch.default.input]
        metrics = [otelcol.processor.batch.default.input]
        traces  = [otelcol.processor.batch.default.input]
      }
    }

    # Batch processor for all telemetry
    otelcol.processor.batch "default" {
      output {
        logs    = [otelcol.exporter.loki.default.input]
        metrics = [prometheus.remote_write.default.receiver]
        traces  = [otelcol.exporter.otlp.default.input]
      }
    }

    # Export logs to Loki
    otelcol.exporter.loki "default" {
      forward_to = [loki.write.default.receiver]
    }
    loki.write "default" {
      endpoint {
        url = "http://loki-gateway.monitoring.svc.cluster.local/loki/api/v1/push"
      }
    }

    # Export metrics to Prometheus (remote_write)
    prometheus.remote_write "mimir" {
      endpoint {
        url = "http://mimir-gateway.monitoring.svc.cluster.local/api/v1/push"
        headers = { "X-Scope-OrgID" = "tenant1" }
      }
    }

    # Export traces to Tempo
    otelcol.exporter.otlp "default" {
      client {
        endpoint = "tempo.monitoring.svc.cluster.local:4317"
      }
    }

  syncPolicy:
    automated: { prune: true, selfHeal: true }
    syncOptions: ["CreateNamespace=true"]
